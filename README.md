# Neural Bits Production Hub
This repository consists of code and articles on the Neural Bits Newsletter that showcase:
- how to optimize, and quantize models for optimal performance
- efficient model serving in production environments at scale
- 
## Categories
### Model Optimization
|ID| Article | Code     | Details | Complexity | Tech Stack |
|--|---------|----------|---------|------------|----------------------|
|001| üìù[Inference Engines Profilling](https://neuralbits.substack.com/p/3-inference-engines-for-optimal-throughput)| üíª[Code](https://github.com/neural-bits/production-hub/tree/main/001-inference_engines) | Profile a CNN model across PyTorch, ONNX, TensorRT, and TorchCompile | Medium |Python, Jupyter|

### Model Deployment
|ID| Article | Code | Details | Complexity | Tech Stack |
|--|---------|------|---------|------------|----------------------|
|002| üìù[Deploying DL models with NVIDIA Triton Inference Server]()| üíª[Code](https://github.com/neural-bits/production-hub/tree/main/002-triton-server-cnn-deployment) | Full tutorial on how to set-up and deploy ML models with Triton Inference Server | Advanced |Python, Docker, Bash|


### Quantization Techniques
|ID| Article | Code | Details | Complexity | Tech Stack |
|--|---------|------|---------|------------|----------------------|
